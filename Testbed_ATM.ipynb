{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist,ngrams,word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "def remove_stopwords(data):\n",
    "    stop_words = Counter(stopwords.words('english'))\n",
    "    ans = []\n",
    "    for each in data:\n",
    "        if(each not in stop_words.keys()):\n",
    "            ans.append(each)\n",
    "    return ans\n",
    "\n",
    "def lemmatizer(data):\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    ans = []\n",
    "    for each in data:\n",
    "        ans.append(lmtzr.lemmatize(each))\n",
    "    return ans\n",
    "\n",
    "def stemmer(data):\n",
    "    ps = PorterStemmer()\n",
    "    ans = []\n",
    "    for each in data:\n",
    "        ans.append(ps.stem(each))\n",
    "    return ans\n",
    "\n",
    "def cleanData(data):\n",
    "    data = word_tokenize(data)\n",
    "    data = lemmatizer(remove_stopwords(data))\n",
    "    string = ' '.join(data)\n",
    "    return data, string\n",
    "\n",
    "\n",
    "def folder_count(path):\n",
    "    count = 0\n",
    "    l = []\n",
    "    for f in os.listdir(path):\n",
    "        child = os.path.join(path,f)\n",
    "        if os.path.isdir(child):\n",
    "            l.append(child)\n",
    "            count += 1\n",
    "    return count, l\n",
    "\n",
    "\n",
    "def create_vocab(dataset_path):\n",
    "    vocab_file = os.path.join(dataset_path,\"vocabulary.txt\")\n",
    "    with open(vocab_file, 'r') as myfile:\n",
    "        data=myfile.read().replace('\\n', ' ')\n",
    "    return data.split(' ')[:-1]\n",
    "\n",
    "def create_tfidf(dataset_path,vocab):\n",
    "    list_docs = []\n",
    "    vectorizer = TfidfVectorizer(stop_words='english',vocabulary=vocab,strip_accents='unicode')\n",
    "    for f in os.listdir(dataset_path):\n",
    "        child = os.path.join(dataset_path,f)\n",
    "        with open(child, 'r', errors='ignore') as myfile:\n",
    "            data=myfile.read().replace('\\n', '')\n",
    "        _ , final_data = cleanData(data)\n",
    "        list_docs.append(final_data)\n",
    "    response = vectorizer.fit_transform(list_docs)\n",
    "    n_response = response.toarray()\n",
    "    row_sum = n_response.sum(axis=1)\n",
    "    length = len(row_sum)\n",
    "    n_result = n_response/row_sum.reshape(length,1)\n",
    "    where_are_NaNs = np.isnan(n_result)\n",
    "    n_result[where_are_NaNs] = 0\n",
    "    n_c_result = sparse.csr_matrix(n_result)\n",
    "#     return response\n",
    "    return n_c_result\n",
    "\n",
    "def sample_document(tfidf_mat):\n",
    "    tfidf_mat = tfidf_mat.transpose()\n",
    "    _,num_docs = tfidf_mat.shape\n",
    "    sampled_document = random.randint(0,num_docs-1)\n",
    "    result = tfidf_mat.getcol(sampled_document).toarray().T\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "use_cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Important model parameters\n",
    "'''\n",
    "DATASET = \"20newsgroups\" # For now, we just test it on 20newsgroups dataset\n",
    "NUM_TOPICS = 20\n",
    "LAMBDA = 10 # Gradient penalty lambda hyperparameter\n",
    "CRITIC_ITERS = 5 # For WGAN and WGAN-GP, number of critic iters per gen iter\n",
    "ITERS = 200000 # How many generator iterations to train for\n",
    "VOCAB_SIZE = 61188# Vocab length of the generator\n",
    "GENERATOR_PARAM = 100 # Number of neurons in the middle layer of the generator\n",
    "LEAK_FACTOR = 0.2 # leak parameter used in generator\n",
    "BATCH_SIZE = 256\n",
    "A_1 = 0.0001\n",
    "B_1 = 0\n",
    "B_2 = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = [np.random.randint(1,11) for i in range(0,20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary change, needs to be changed later\n",
    "dataset_path = \"/home/ysahil/Academics/Sem_8/ATM_GANs/20news/20news-18828/all_docs\"\n",
    "dataset_path_1 = \"/home/ysahil/Academics/Sem_8/ATM_GANs/20news/20news-18828\"\n",
    "\n",
    "#Create the TF-IDF matrix\n",
    "def get_tfidf():\n",
    "    vocab = create_vocab(dataset_path_1)\n",
    "    result = create_tfidf(dataset_path,vocab)\n",
    "    return result\n",
    "\n",
    "## TODO: Incorporate normalization of tf-idf matrix over row-sum\n",
    "def representation_map(result):\n",
    "#    vocab = create_vocab(dataset_path_1)\n",
    "#    result = create_tfidf(dataset_path,vocab)\n",
    "    sam_doc = sample_document(result)\n",
    "    #print(sam_doc)\n",
    "    #print(sam_doc.shape)\n",
    "    return sam_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = get_tfidf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "representation_map(test_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions of GAN's and the training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(generator,self).__init__()\n",
    "        main = nn.Sequential(\n",
    "               nn.Linear(NUM_TOPICS,GENERATOR_PARAM),\n",
    "               nn.LeakyReLU(LEAK_FACTOR,True),\n",
    "               nn.BatchNorm1d(GENERATOR_PARAM),\n",
    "               nn.Linear(GENERATOR_PARAM,VOCAB_SIZE),\n",
    "#                nn.Softmax(VOCAB_SIZE)\n",
    "               nn.Softmax()\n",
    "               )\n",
    "        self.main = main\n",
    "\n",
    "    def forward(self,noise):\n",
    "        output = self.main(noise)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(discriminator,self).__init__()\n",
    "        main = nn.Sequential(\n",
    "               nn.Linear(VOCAB_SIZE,GENERATOR_PARAM),\n",
    "               nn.LeakyReLU(LEAK_FACTOR,True),\n",
    "               nn.Linear(GENERATOR_PARAM,1))\n",
    "        self.main = main\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        output = self.main(inputs)\n",
    "        return output.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(v):\n",
    "    return v.data.storage().tolist()\n",
    "\n",
    "def stats(d):\n",
    "    return [np.mean(d), np.std(d)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inf_data_gen(alpha):\n",
    "    if DATASET == \"20newsgroups\":\n",
    "        while True:\n",
    "            dataset = []\n",
    "            for i in range(BATCH_SIZE):\n",
    "                sample = np.random.dirichlet(alpha)\n",
    "                dataset.append(sample)\n",
    "            dataset = np.array(dataset, dtype='float32')\n",
    "            np.random.shuffle(dataset)\n",
    "            yield dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_data_sampler(test_result):\n",
    "    while True:\n",
    "        dataset = []\n",
    "        for i in range(BATCH_SIZE):\n",
    "            sample = np.array(representation_map(test_result))\n",
    "            dataset.append(sample[0])\n",
    "        dataset = np.array(dataset, dtype='float32')\n",
    "        np.random.shuffle(dataset)\n",
    "        yield dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_gradient_penalty(netD, real_data, fake_data):\n",
    "    alpha = torch.rand(BATCH_SIZE, 1)\n",
    "    alpha = alpha.expand(real_data.size())\n",
    "    alpha = alpha.cuda() if use_cuda else alpha\n",
    "\n",
    "    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "\n",
    "    if use_cuda:\n",
    "        interpolates = interpolates.cuda()\n",
    "    interpolates = autograd.Variable(interpolates, requires_grad=True)\n",
    "\n",
    "    disc_interpolates = netD(interpolates)\n",
    "\n",
    "    gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n",
    "                              grad_outputs=torch.ones(disc_interpolates.size()).cuda() if use_cuda else torch.ones(\n",
    "                                  disc_interpolates.size()),\n",
    "                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * LAMBDA\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generator(\n",
      "  (main): Sequential(\n",
      "    (0): Linear(in_features=20, out_features=100, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Linear(in_features=100, out_features=61188, bias=True)\n",
      "    (4): Softmax()\n",
      "  )\n",
      ")\n",
      "discriminator(\n",
      "  (main): Sequential(\n",
      "    (0): Linear(in_features=61188, out_features=100, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace)\n",
      "    (2): Linear(in_features=100, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "ATM_G = generator()\n",
    "ATM_D = discriminator()\n",
    "ATM_G.apply(weights_init)\n",
    "ATM_D.apply(weights_init)\n",
    "print(ATM_G)\n",
    "print(ATM_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_cuda:\n",
    "    ATM_D = ATM_D.cuda()\n",
    "    ATM_G = ATM_G.cuda()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizerD = optim.Adam(ATM_D.parameters(), lr=A_1, betas=(B_1, B_2))\n",
    "optimizerG = optim.Adam(ATM_G.parameters(), lr=A_1, betas=(B_1, B_2))\n",
    "\n",
    "one = torch.FloatTensor([1])\n",
    "# mone = one * -1\n",
    "mone = torch.FloatTensor([0])\n",
    "if use_cuda:\n",
    "    one = one.cuda()\n",
    "    mone = mone.cuda()\n",
    "\n",
    "data = inf_data_gen(alpha)\n",
    "real_data = real_data_sampler(test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ysahil/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n",
      "199\n",
      "299\n",
      "399\n",
      "499\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-4fde93a85e32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# train with gradient penalty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mgradient_penalty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_gradient_penalty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mATM_D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampled_real_data_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mgradient_penalty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mD_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD_fake\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mD_real\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgradient_penalty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for iteration in range(ITERS):\n",
    "    ############################\n",
    "    # (1) Update D network\n",
    "    ###########################\n",
    "    for p in ATM_D.parameters():  # reset requires_grad\n",
    "        p.requires_grad = True  # they are set to False below in netG update\n",
    "\n",
    "    for iter_d in range(CRITIC_ITERS):\n",
    "        #_data = data.next()\n",
    "        _data = next(data)\n",
    "        sampled_data = torch.Tensor(_data)\n",
    "        if use_cuda:\n",
    "            sampled_data = sampled_data.cuda()\n",
    "        sampled_data_v = autograd.Variable(sampled_data)\n",
    "\n",
    "        #print(sampled_data_v.size())\n",
    "         # train with sampled(fake data)\n",
    "        fake = autograd.Variable(ATM_G(sampled_data_v).data)\n",
    "        D_fake = ATM_D(fake)\n",
    "        D_fake = D_fake.mean()\n",
    "        D_fake.backward(one)\n",
    "#         D_fake_error = criterion(D_fake,autograd.Variable(one))\n",
    "#         D_fake_error.backward()\n",
    "\n",
    "        #_realdata = real_data.next()\n",
    "        _realdata = next(real_data)\n",
    "        sampled_real_data = torch.Tensor(_realdata)\n",
    "        if use_cuda:\n",
    "            sampled_real_data = sampled_real_data.cuda()\n",
    "        sampled_real_data_v = autograd.Variable(sampled_real_data)\n",
    "\n",
    "        D_real = ATM_D(sampled_real_data_v)\n",
    "        D_real = D_real.mean()\n",
    "        D_real.backward(mone)\n",
    "#         D_real_error = criterion(D_real,autograd.Variable(mone))\n",
    "#         D_real_error.backward()\n",
    "\n",
    "        #print(sampled_real_data_v.size())\n",
    "        # train with gradient penalty\n",
    "        gradient_penalty = calc_gradient_penalty(ATM_D, sampled_real_data_v.data, fake.data)\n",
    "        gradient_penalty.backward()\n",
    "\n",
    "        D_cost = D_fake - D_real + gradient_penalty\n",
    "        Wasserstein_D = D_real - D_fake\n",
    "        optimizerD.step()\n",
    "\n",
    "#         dre, dfe = extract(D_real_error)[0], extract(D_fake_error)[0]\n",
    "\n",
    "    for p in ATM_D.parameters():\n",
    "        p.requires_grad = False  # to avoid computation\n",
    "    ATM_G.zero_grad()\n",
    "\n",
    "    #_data = data.next()\n",
    "    _data = next(data)\n",
    "    sampled_data = torch.Tensor(_data)\n",
    "    if use_cuda:\n",
    "        sampled_data = sampled_data.cuda()\n",
    "    sampled_data_v = autograd.Variable(sampled_data)\n",
    "\n",
    "    fake = ATM_G(sampled_data_v)\n",
    "    G = ATM_D(fake)\n",
    "    G = G.mean()\n",
    "    G.backward(mone)\n",
    "#     G_error = criterion(G,autograd.Variable(mone))\n",
    "#     G_error.backward()\n",
    "#     ge = extract(G_error)[0]\n",
    "    G_cost = -G\n",
    "    optimizerG.step()\n",
    "    if iteration % 100 == 99:\n",
    "        print(iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
